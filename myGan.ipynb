{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/')\n",
    "\n",
    "batch_size = 64\n",
    "z_dimension = 128\n",
    "\n",
    "lower_bound = 0.01\n",
    "upper_bound = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x,alpha = 0.2):\n",
    "    return tf.maximum(x, alpha * x)\n",
    "\n",
    "def batch_norm(input,isTraining,decay = 0.999):\n",
    "    #this is for conv only\n",
    "    epsilon = 1e-5\n",
    "    shape = input.get_shape().as_list()\n",
    "    scale = tf.get_variable('scale',[shape[3]],initializer = tf.constant_initializer(1))\n",
    "    offset = tf.get_variable('offset',[shape[3]],initializer = tf.constant_initializer(0))\n",
    "    \n",
    "    pop_mean = tf.get_variable('pop_mean',[shape[3]],initializer = tf.constant_initializer(0),trainable = False)\n",
    "    pop_var = tf.get_variable('pop_var',[shape[3]],initializer = tf.constant_initializer(1),trainable = False)\n",
    "    \n",
    "    if isTraining:\n",
    "        batch_mean, batch_var = tf.nn.moments(input,[0,1,2])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(input,\n",
    "                batch_mean, batch_var, offset, scale, epsilon)\n",
    "    \n",
    "    return tf.nn.batch_normalization(input,\n",
    "                pop_mean, pop_var, offset, scale, epsilon)\n",
    "\n",
    "def conv2d(input,kernal_shape,isTraining):\n",
    "    weights = tf.get_variable('weights',kernal_shape,initializer = tf.variance_scaling_initializer())\n",
    "    #xavier initializer set scale factor as 2 for relu\n",
    "    #biases = tf.get_variable('biases',bias_shape,initializer = tf.constant_initializer(0))\n",
    "    #comment out biases since batch_norm is applied\n",
    "    \n",
    "    output = tf.nn.conv2d(input = input, filter = weights, strides = [1,1,1,1], padding = 'SAME')\n",
    "    output = batch_norm(output,isTraining)\n",
    "    output = leaky_relu(output)\n",
    "    output = tf.nn.avg_pool(output,ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    return output\n",
    "\n",
    "def deconv2d(input,kernal_shape,output_shape,isTraining,norm = True):\n",
    "    weights_de = tf.get_variable('weights_de',kernal_shape,initializer = tf.variance_scaling_initializer())\n",
    "    \n",
    "    output = tf.nn.conv2d_transpose(value = input, filter = weights_de,output_shape = output_shape,strides = [1,2,2,1], padding = 'SAME')\n",
    "    if norm:\n",
    "        output = batch_norm(output,isTraining)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def fcn(input,input_size,output_size,isTraining,norm = True):\n",
    "    wf = tf.get_variable('wf',[input_size,output_size],initializer = tf.variance_scaling_initializer())\n",
    "    if norm:\n",
    "        output = batch_norm(tf.matmul(input,wf),isTraining)\n",
    "    else:\n",
    "        bf = tf.get_variable('bf',[output_size],initializer = tf.constant_initializer(0))\n",
    "        output = tf.matmul(input,wf) + bf\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(images,ifReuse = False,isTraining = True):\n",
    "    with tf.variable_scope('discriminator',reuse = ifReuse):\n",
    "        with tf.variable_scope('conv1'):\n",
    "            d = conv2d(images,[5,5,1,32],isTraining)\n",
    "        with tf.variable_scope('conv2'):\n",
    "            d = conv2d(d,[5,5,32,64],isTraining)\n",
    "        with tf.variable_scope('fcnd1'):\n",
    "            d = tf.reshape(d,[-1,7 * 7 * 64])\n",
    "            d = fcn(d,7 * 7 * 64,1024,isTraining,norm = False)\n",
    "            d = leaky_relu(d)\n",
    "        with tf.variable_scope('fcnd2'):\n",
    "            d = fcn(d,1024,1,isTraining,norm = False)#logits\n",
    "            p = tf.sigmoid(d)\n",
    "    return d,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(input,input_dimension,batch_size = 64,isTraining = True):\n",
    "    with tf.variable_scope('generator'):\n",
    "        with tf.variable_scope('fcng1'):\n",
    "            d = fcn(input,input_dimension,7 * 7 * 64,isTraining,norm = False)\n",
    "            d = leaky_relu(d)\n",
    "            d = tf.reshape(d,[-1,7,7,64])\n",
    "        with tf.variable_scope('deconv1'):\n",
    "            d = deconv2d(d,[5,5,32,64],[batch_size,14,14,32],isTraining)\n",
    "            d = leaky_relu(d)\n",
    "        with tf.variable_scope('deconv2'):\n",
    "            d = deconv2d(d,[5,5,1,32],[batch_size,28,28,1],isTraining,norm = False)\n",
    "            d = tf.tanh(d)\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_input = tf.placeholder(tf.float32, [None,z_dimension])\n",
    "d_input = tf.placeholder(tf.float32, [None,28,28,1])\n",
    "\n",
    "#generate a batch of fake images\n",
    "z_output = generator(z_input,z_dimension)\n",
    "#judge the real images\n",
    "d_output_real,p_real = discriminator(d_input)\n",
    "#judge the fake images\n",
    "d_output_fake,p_fake = discriminator(z_output,ifReuse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'WGan'\n",
    "if mode == 'vanilla':\n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = d_output_real, labels = tf.ones_like(d_output_real)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = d_output_fake, labels = tf.zeros_like(d_output_fake)))\n",
    "\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = d_output_fake, labels = tf.ones_like(d_output_fake)))\n",
    "else:\n",
    "    d_loss = tf.reduce_mean(d_output_fake - d_output_real)\n",
    "    g_loss = tf.reduce_mean(-d_output_fake)\n",
    "\n",
    "d_correct_real = p_real > 0.5\n",
    "d_correct_fake = p_fake > 0.5\n",
    "\n",
    "accuracy_real = tf.reduce_mean(tf.cast(d_correct_real,tf.float32))\n",
    "accuracy_fake = tf.reduce_mean(tf.cast(d_correct_fake,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "\n",
    "gvars = [var for var in tvars if 'generator' in var.name]\n",
    "dvars = [var for var in tvars if 'discriminator' in var.name]\n",
    "\n",
    "if mode == 'vanilla':\n",
    "    d_trainer = tf.train.GradientDescentOptimizer(0.001).minimize(d_loss, var_list=dvars)\n",
    "    g_trainer = tf.train.AdamOptimizer(0.0003).minimize(g_loss, var_list=gvars)\n",
    "else:\n",
    "    d_trainer = tf.train.RMSPropOptimizer(0.0003).minimize(d_loss, var_list=dvars)\n",
    "    g_trainer = tf.train.RMSPropOptimizer(0.0003).minimize(g_loss, var_list=gvars)\n",
    "    with tf.control_dependencies([d_trainer]):\n",
    "        clip = (tf.tuple([tf.assign(var, tf.clip_by_value(var, lower_bound, upper_bound)) for var in dvars]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide infos for TensorBoard\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "tf.summary.scalar('Generator_loss', g_loss)\n",
    "tf.summary.scalar('Discriminator_loss_real', d_loss)\n",
    "tf.summary.scalar('real_accuracy', accuracy_real)\n",
    "tf.summary.scalar('fake_accuracy', accuracy_fake)\n",
    "\n",
    "images_for_tensorboard = generator(z_input, z_dimension,batch_size,False)\n",
    "tf.summary.image('Generated_images', images_for_tensorboard, 5)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start training\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "writer.add_graph(sess.graph)\n",
    "print(datetime.datetime.now())\n",
    "for i in range(100000):\n",
    "    #normalize images data from [0,1] to [-1,1]\n",
    "    real_images = (mnist.train.next_batch(batch_size)[0].reshape([batch_size, 28, 28, 1]) - 0.5) * 2\n",
    "    z_noise = np.random.normal(-1,1,[batch_size,z_dimension])\n",
    "    # Train discriminator on both real and fake images\n",
    "    _,ac_real,ac_fake = sess.run([d_trainer,accuracy_real, accuracy_fake],\n",
    "                                           {z_input: z_noise, d_input: real_images})\n",
    "\n",
    "    # Train generator\n",
    "    z_noise = np.random.normal(-1,1,[batch_size,z_dimension])\n",
    "    g_trained = sess.run(g_trainer,feed_dict={z_input: z_noise})\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        # Update TensorBoard with summary statistics\n",
    "        z_noise = np.random.normal(-1,1,[batch_size,z_dimension])\n",
    "        summary = sess.run(merged, {z_input: z_noise, d_input: real_images})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        # Every 100 iterations, show a generated image\n",
    "        print(\"Iteration:\", i, \"at\", datetime.datetime.now())\n",
    "        z_noise = np.random.normal(-1,1,[1,z_dimension])\n",
    "        generated_images = generator(z_input, z_dimension,1,False)\n",
    "        images = sess.run(generated_images, {z_input: z_noise})\n",
    "        plt.imshow(images[0].reshape([28, 28]), cmap='Greys')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "#save the model\n",
    "\n",
    "\n",
    "save_path = saver.save(sess, \"./model/model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
